# $$Lecture 4$$
$$rrison$$
## 2 图像去畸变
见code/undistort/
## 3 双目视差
见code/stereo
## 4 矩阵微分
1. $d(Ax)/dx$是实值向量函数的行向量偏导数，称之为向量函数$f(x)$在$x$处的Jacobian矩阵
2. $d(x^TAx)/dx$是实值标量函数的行向量偏导数，称之为实值标量函数$f(x)$在$x$处的梯度向量
3. 证明$x^TAx=tr(Axx^T)$

$x^TAx=[x_1\ x_2\ \dots x_n]\cdot \begin{bmatrix}
   a_{11} & a_{12} & \dots & a_{1n} \\\\
   a_{21} & a_{22} & \dots & a_{2n} \\\\
   \dots & \dots & \dots & \dots \\\\
   a_{n1} & a_{n2} & \dots & a_{nn} \\\\
  \end{bmatrix}\cdot \begin{bmatrix}
   x_1 \\\\
   x_2 \\\\
   \dots \\\\
   x_n
  \end{bmatrix}$
  
$=[x_1\ x_2\ \dots x_n]\cdot \begin{bmatrix}
   \sum_{i=1}^n a_{1i}x_i \\\\
   \sum_{i=1}^n a_{2i}x_i \\\\
   \dots \\\\
   \sum_{i=1}^n a_{ni}x_i \\\\
  \end{bmatrix}$
  
  $=x_1\sum_{i=1}^n a_{1i}x_i+x_2\sum_{i=1}^n a_{2i}x_i+\cdots +x_n\sum_{i=1}^n a_{ni}x_i$


$tr(Axx^T)=tr\left(\begin{bmatrix}
   a_{11} & a_{12} & \dots & a_{1n} \\\\
   a_{21} & a_{22} & \dots & a_{2n} \\\\
   \dots & \dots & \dots & \dots \\\\
   a_{n1} & a_{n2} & \dots & a_{nn} \\\\
  \end{bmatrix}\cdot[x_1\ x_2\ \dots x_n] \cdot \begin{bmatrix}
   x_1 \\\\
   x_2 \\\\
   \dots \\\\
   x_n
  \end{bmatrix}\right)$
  
  $=tr\left(\begin{bmatrix}
   a_{11} & a_{12} & \dots & a_{1n} \\\\
   a_{21} & a_{22} & \dots & a_{2n} \\\\
   \dots & \dots & \dots & \dots \\\\
   a_{n1} & a_{n2} & \dots & a_{nn} \\\\
  \end{bmatrix}\cdot \begin{bmatrix}
   x_1x_1 & x_1x_2 & \dots & x_1x_n \\\\
   x_2x_1 & x_2x_2 & \dots & x_nx_n \\\\
   \dots & \dots & \dots & \dots \\\\
   x_nx_1 & x_nx_2 & \dots & x_nx_n \\\\
  \end{bmatrix}\right)$
  
  $=\sum_{i=1}^n a_{1i}x_ix_1+\sum_{i=1}^n a_{2i}x_ix_2+\cdots +\sum_{i=1}^n a_{ni}x_ix_n$
  
  $=x_1\sum_{i=1}^n a_{1i}x_i+x_2\sum_{i=1}^n a_{2i}x_i+\cdots +x_n\sum_{i=1}^n a_{ni}x_i$
  
所以得到$x^TAx=tr(Axx^T)=x_1\sum_{i=1}^n a_{1i}x_i+x_2\sum_{i=1}^n a_{2i}x_i+\cdots +x_n\sum_{i=1}^n a_{ni}x_i$
## 5 高斯牛顿法
见code/gaussnewton
